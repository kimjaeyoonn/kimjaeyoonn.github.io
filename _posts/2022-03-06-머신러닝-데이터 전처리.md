---
title: '머신러닝 - 데이터 전처리' 
excerpt: "혼자 공부하는 머신러닝 - 데이터 전처리"
categories: Machine-Learning
author_profile: true    #작성자 프로필 출력 여부

last_modified_at: 2022-03-06 T21:00:00+09:00

toc: true   #Table Of Contents 목차 

toc_sticky: true
---



## 넘파이로 데이터 준비하기


```python
fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 
                10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 
                7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
```


```python
import numpy as np
```


```python
# column_stack() 함수는 전달받은 리스트를 일렬로 세운 다음 차례대로 나란히 연결한다. 
np.column_stack(([1,2,3],[4,5,6]))
```




    array([[1, 4],
           [2, 5],
           [3, 6]])




```python
# input 데이터 생성
fish_data = np.column_stack((fish_length, fish_weight))
print(fish_data[:5])
```

    [[ 25.4 242. ]
     [ 26.3 290. ]
     [ 26.5 340. ]
     [ 29.  363. ]
     [ 29.  430. ]]



```python
# target 데이터 생성
fish_target = np.concatenate((np.ones(35), np.zeros(14)))
print(fish_target)
```

    [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
     1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
     0.]


## 사이킷런으로 훈련 세트와 테스트 세트 나누기


```python
from sklearn.model_selection import train_test_split
```


```python
# train_test_split() 함수에는 random_state 매개변수가 있다.
# random_state는 랜덤 시드를 말하며 42개로 지정하였다. 
train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state=42)
```


```python
# 잘 나누었는지 numpy의 shape 속성으로 확인.
print(train_input.shape, test_input.shape)
print(train_target.shape, test_target.shape)
```

    (36, 2) (13, 2)
    (36,) (13,)



```python
# 도미와 빙어가 잘 섞였는지 확인하기 위해 test_target 데이터를 출력
print(test_target)
# 골고루 잘 섞이지 않고 샘플링 편향이 나타난 것을 볼 수 있다. 
```

    [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]



```python
# train_test_split() 함수에는 stratify 매개변수를 통해 샘플링 편향을 해결할 수 있다.
train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify=fish_target, random_state=42)
print(test_target)
```

    [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.]


## 훈련 데이터로 모델 훈련 / 테스트 데이터로 모델 평가


```python
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier()
kn.fit(train_input, train_target)
kn.score(test_input, test_target)
```




    1.0



KNN을 통해 train set을 fit(훈련) 후 test set을 score(평가) > 완벽한 정확도 (1.0) 


```python
print(kn.predict([[25, 150]]))
```

    [0.]


- 당연히 올바르게 예측해야 할 도미(1) 데이터를 잘못 예측했다.
- 산점도를 통해 잘못 예측한 데이터 값을 식별해본다.


```python
import matplotlib.pyplot as plt
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^') 
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


![output_17_0.png](https://github.com/kimjaeyoonn/kimjaeyoonn.github.io/blob/master/_images/%ED%98%BC%EA%B3%B5%EB%A8%B8-2.%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A0%84%EC%B2%98%EB%A6%AC/output_17_0.png?raw=true)
    



```python
# kneighbors() 메소드는 이웃까지의 거리와 이웃 샘플의 인덱스를 반환합니다.
distances, indexes = kn.kneighbors([[25, 150]])
```


```python
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^') 
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker="D") # kneighbors 샘플 5개
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


![output_19_0.png](https://github.com/kimjaeyoonn/kimjaeyoonn.github.io/blob/master/_images/%ED%98%BC%EA%B3%B5%EB%A8%B8-2.%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A0%84%EC%B2%98%EB%A6%AC/output_19_0.png?raw=true)
    



```python
print(train_input[indexes])
print(train_target[indexes]) 
```

    [[[ 25.4 242. ]
      [ 15.   19.9]
      [ 14.3  19.7]
      [ 13.   12.2]
      [ 12.2  12.2]]]
    [[1. 0. 0. 0. 0.]]


- 왜 가장 가까운 5개의 neighbors를 빙어라고 생각한 것일까? 눈에 보이기에는 도미와 더 가까운데!
- 이유는 x축의 범위가 좁고, y축의 범위가 넓기 때문이다. (두 특성의 스케일이 다르다.)
- 아래의 코드처럼 두 축의 범위 기준을 맞춰야 한다.


```python
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^') 
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker="D") # kneighbors 샘플 5개
plt.xlim((0,1000))
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


![output_22_0.png](https://github.com/kimjaeyoonn/kimjaeyoonn.github.io/blob/master/_images/%ED%98%BC%EA%B3%B5%EB%A8%B8-2.%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A0%84%EC%B2%98%EB%A6%AC/output_22_0.png?raw=true)
    


- 모든 모델의 알고리즘이 KNN처럼 거리를 기반으로 하는 것은 아니다.


## 표준점수를 활용한 전처리

가장 널리 사용하는 전처리 방법 중 하나는 표준점수이다. 각 특성값이 평균에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다. 이를 통해 실제 특성값의 크기와 상관없이 동일한 조건으로 비교할 수 있다.
- 계산 방법 : 평균을 빼고 표준편차를 나누어주면 된다. numpy는 이 두 함수를 모두 제공한다. 


```python
# 각 특성별로 계산하기 위해 axis=0으로 지정 (행을 따라 열의 통계 값 계산)
mean = np.mean(train_input, axis=0)  # 평균 계산
std = np.std(train_input, axis=0)  # 표준편차 계산
print(mean, std)
```

    [ 27.29722222 454.09722222] [  9.98244253 323.29893931]



```python
# numpy의 '브로드캐스팅' 기능 각각의 행에 대한 계산
train_scaled = (train_input - mean) / std
```

## 표준점수 데이터로 모델 훈련하기


```python
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(25, 150, marker='^') 
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


![output_29_0.png](https://github.com/kimjaeyoonn/kimjaeyoonn.github.io/blob/master/_images/%ED%98%BC%EA%B3%B5%EB%A8%B8-2.%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A0%84%EC%B2%98%EB%A6%AC/output_29_0.png?raw=true)
    


샘플[25, 150]은 동일한 비율로 변환하지 않아서 덩그러니 떨어진 위치에 표시됨.


```python
new = ([25, 150] - mean) / std
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


![output_31_0.png](https://github.com/kimjaeyoonn/kimjaeyoonn.github.io/blob/master/_images/%ED%98%BC%EA%B3%B5%EB%A8%B8-2.%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A0%84%EC%B2%98%EB%A6%AC/output_31_0.png?raw=true)
    


축의 범위가 달라지면서, 샘플 데이터가 비슷한 범위를 차지하게 됨을 확인할 수 있다.


```python
# train_scaled 데이터 셋으로 KNN 모델을 훈련 !
kn.fit(train_scaled, train_target)
```




    KNeighborsClassifier()



## 모델 평가하기 


```python
# test set도 train set의 평균과 표준편차로 변환해야 한다. (브로드 캐스팅)
test_scaled = (test_input - mean) / std
```


```python
kn.score(test_scaled, test_target)
```




    1.0




```python
# 이전에 뽑은 샘플 예측
print(kn.predict([new]))
```

    [1.]



```python
# kneighbors() 함수로 new 샘플의 이웃을 확인 !!!
distances, indexes = kn.kneighbors([new])
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker='D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


​    
![output_38_0.png](https://github.com/kimjaeyoonn/kimjaeyoonn.github.io/blob/master/_images/%ED%98%BC%EA%B3%B5%EB%A8%B8-2.%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%A0%84%EC%B2%98%EB%A6%AC/output_38_0.png?raw=true)
​    


표준점수 데이터셋으로 전처리하고 훈련 및 평가한 결과 두 특성값이 scale에 민감하지 않고 안정적인 예측을 할 수 있음을 볼 수 있다.
